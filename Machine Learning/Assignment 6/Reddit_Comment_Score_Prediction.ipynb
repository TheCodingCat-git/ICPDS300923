{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7tI9vBmv9C2"
      },
      "source": [
        "## Assignment-6: Machine Learning on Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reddit Comment Score Prediction**\n",
        "\n",
        "Reddit is a social news platform that allows users to discuss and vote on content that other users have submitted.On an average reddit receives 470,000 comments per day. The comments are further upvoted or downvoted by the registered users. \n",
        "\n",
        "Imagine you are going to start a forum where users can post or comment or share content on the platform. Now you want to filter out some positive comments and recommend them to your users. \n",
        "\n",
        "\n",
        "Build a machine learning model that will help you know which comment or content is going to be popular in the near future (the content which receives the highest upvotes will be popular) and accordingly recommend such content to your users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflowNote: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Using cached tensorflow-2.15.0-cp311-cp311-win_amd64.whl (2.1 kB)\n",
            "Collecting tensorflow-intel==2.15.0 (from tensorflow)\n",
            "  Using cached tensorflow_intel-2.15.0-cp311-cp311-win_amd64.whl (300.9 MB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting flatbuffers>=23.5.26 (from tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Collecting h5py>=2.9.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached h5py-3.10.0-cp311-cp311-win_amd64.whl (2.7 MB)\n",
            "Collecting libclang>=13.0.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl (938 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.25.2)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.24.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached wrapt-1.14.1-cp311-cp311-win_amd64.whl (35 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached grpcio-1.62.0-cp311-cp311-win_amd64.whl (3.8 MB)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached wheel-0.42.0-py3-none-any.whl (65 kB)\n",
            "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached google_auth-2.28.1-py2.py3-none-any.whl (186 kB)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.1)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.3)\n",
            "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
            "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
            "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "Installing collected packages: libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, oauthlib, ml-dtypes, markdown, keras, h5py, grpcio, google-pasta, gast, absl-py, rsa, requests-oauthlib, pyasn1-modules, astunparse, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
            "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.28.1 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 grpcio-1.62.0 h5py-3.10.0 keras-2.15.0 libclang-16.0.6 markdown-3.5.2 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-intel-2.15.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0 werkzeug-3.0.1 wheel-0.42.0 wrapt-1.14.1\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_hubNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_hub) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_hub) (4.24.3)\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow_hub)\n",
            "  Downloading tf_keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "                                              0.0/1.7 MB ? eta -:--:--\n",
            "     -----                                    0.2/1.7 MB 3.8 MB/s eta 0:00:01\n",
            "     --------                                 0.4/1.7 MB 2.3 MB/s eta 0:00:01\n",
            "     -------------                            0.6/1.7 MB 2.3 MB/s eta 0:00:01\n",
            "     -----------------------                  1.0/1.7 MB 2.8 MB/s eta 0:00:01\n",
            "     --------------------------------         1.4/1.7 MB 3.2 MB/s eta 0:00:01\n",
            "     -------------------------------------    1.6/1.7 MB 3.3 MB/s eta 0:00:01\n",
            "     -------------------------------------    1.6/1.7 MB 3.3 MB/s eta 0:00:01\n",
            "     -------------------------------------    1.6/1.7 MB 3.3 MB/s eta 0:00:01\n",
            "     -------------------------------------    1.6/1.7 MB 3.3 MB/s eta 0:00:01\n",
            "     -------------------------------------    1.6/1.7 MB 3.3 MB/s eta 0:00:01\n",
            "     -------------------------------------    1.6/1.7 MB 3.3 MB/s eta 0:00:01\n",
            "     ---------------------------------------  1.7/1.7 MB 2.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------  1.7/1.7 MB 2.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 1.7/1.7 MB 1.9 MB/s eta 0:00:00\n",
            "Installing collected packages: tf-keras, tensorflow_hub\n",
            "Successfully installed tensorflow_hub-0.16.1 tf-keras-2.15.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-06-07T10:45:25.731680Z",
          "iopub.status.busy": "2021-06-07T10:45:25.731267Z",
          "iopub.status.idle": "2021-06-07T10:45:25.746854Z",
          "shell.execute_reply": "2021-06-07T10:45:25.745721Z",
          "shell.execute_reply.started": "2021-06-07T10:45:25.731642Z"
        },
        "id": "8dLxtUgOv9C4",
        "outputId": "92bc5aac-9d35-4352-a3d0-c781ac550df7",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk[twitter] in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk[twitter]) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk[twitter]) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk[twitter]) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk[twitter]) (4.66.2)\n",
            "Collecting twython (from nltk[twitter])\n",
            "  Downloading twython-3.9.1-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk[twitter]) (0.4.6)\n",
            "Requirement already satisfied: requests>=2.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from twython->nltk[twitter]) (2.31.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from twython->nltk[twitter]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.1.0->twython->nltk[twitter]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.1.0->twython->nltk[twitter]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.1.0->twython->nltk[twitter]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.1.0->twython->nltk[twitter]) (2023.7.22)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-oauthlib>=0.4.0->twython->nltk[twitter]) (3.2.2)\n",
            "Installing collected packages: twython\n",
            "Successfully installed twython-3.9.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
            "                                              0.0/991.5 kB ? eta -:--:--\n",
            "     ---                                     81.9/991.5 kB 2.3 MB/s eta 0:00:01\n",
            "     ---------                              245.8/991.5 kB 3.0 MB/s eta 0:00:01\n",
            "     ----------------                       440.3/991.5 kB 3.1 MB/s eta 0:00:01\n",
            "     -------------------------              665.6/991.5 kB 3.5 MB/s eta 0:00:01\n",
            "     ------------------------------         798.7/991.5 kB 3.6 MB/s eta 0:00:01\n",
            "     -------------------------------------  983.0/991.5 kB 3.5 MB/s eta 0:00:01\n",
            "     -------------------------------------  983.0/991.5 kB 3.5 MB/s eta 0:00:01\n",
            "     -------------------------------------  983.0/991.5 kB 3.5 MB/s eta 0:00:01\n",
            "     -------------------------------------  983.0/991.5 kB 3.5 MB/s eta 0:00:01\n",
            "     -------------------------------------  983.0/991.5 kB 3.5 MB/s eta 0:00:01\n",
            "     -------------------------------------  983.0/991.5 kB 3.5 MB/s eta 0:00:01\n",
            "     -------------------------------------  983.0/991.5 kB 3.5 MB/s eta 0:00:01\n",
            "     -------------------------------------- 991.5/991.5 kB 1.6 MB/s eta 0:00:00\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.2.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import os\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "!pip3 install -U nltk[twitter] \n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T10:46:17.963165Z",
          "iopub.status.busy": "2021-06-07T10:46:17.962851Z",
          "iopub.status.idle": "2021-06-07T10:46:18.020938Z",
          "shell.execute_reply": "2021-06-07T10:46:18.019849Z",
          "shell.execute_reply.started": "2021-06-07T10:46:17.963138Z"
        },
        "id": "_vEWnSnjv9C5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df_compi = pd.read_csv(\"Train_Data.csv\")\n",
        "df_compi_test = pd.read_csv(\"Test_Data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-06-07T10:32:57.230758Z",
          "iopub.status.busy": "2021-06-07T10:32:57.230462Z",
          "iopub.status.idle": "2021-06-07T10:32:57.257956Z",
          "shell.execute_reply": "2021-06-07T10:32:57.256260Z",
          "shell.execute_reply.started": "2021-06-07T10:32:57.230729Z"
        },
        "id": "DmpGnnEbv9C6",
        "outputId": "2b359b0d-1305-43d6-ceab-7bc1548e801d",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "positive\n",
              "1    2504\n",
              "0    2495\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# mark postive and negative sentiment\n",
        "df_compi[\"positive\"] = 1\n",
        "df_compi.loc[(df_compi.Score <0), \"positive\"] = 0\n",
        "df_compi.positive.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgO-BrGTv9C7"
      },
      "source": [
        "# Dropping duplicate columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-06-07T10:32:57.270212Z",
          "iopub.status.busy": "2021-06-07T10:32:57.269895Z",
          "iopub.status.idle": "2021-06-07T10:32:57.284874Z",
          "shell.execute_reply": "2021-06-07T10:32:57.282912Z",
          "shell.execute_reply.started": "2021-06-07T10:32:57.270182Z"
        },
        "id": "qzp9_4Oqv9C7",
        "outputId": "0ef3f0fa-797f-4033-81a8-29ea4ae3294d",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check if \"parent_votes\" and \"parent_score\" columns contains the same value\n",
        "(df_compi.parent_votes == df_compi.parent_score).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T10:32:57.324226Z",
          "iopub.status.busy": "2021-06-07T10:32:57.323800Z",
          "iopub.status.idle": "2021-06-07T10:32:59.980843Z",
          "shell.execute_reply": "2021-06-07T10:32:59.980097Z",
          "shell.execute_reply.started": "2021-06-07T10:32:57.324182Z"
        },
        "id": "BFc9YtQHv9C8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# drop duplicate and redundunt columns\n",
        "red_col_list = [\"author\", \"parent_author\"]\n",
        "df_compi.drop([\"parent_votes\"] + red_col_list, axis= 1,inplace= True)\n",
        "df_compi_test.drop([\"parent_votes\"] + red_col_list, axis= 1,inplace= True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdPvXE0nv9C8"
      },
      "source": [
        "# Adding VADER polarity scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T10:33:13.306556Z",
          "iopub.status.busy": "2021-06-07T10:33:13.306262Z",
          "iopub.status.idle": "2021-06-07T10:33:14.882413Z",
          "shell.execute_reply": "2021-06-07T10:33:14.881316Z",
          "shell.execute_reply.started": "2021-06-07T10:33:13.306526Z"
        },
        "id": "zcbXH-2wv9C8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def add_polarity_score(df):\n",
        "    \"\"\"\n",
        "    Add sentiment scores\n",
        "    \"\"\"\n",
        "    # text column names\n",
        "    text_col = [\"text\", \"parent_text\"]\n",
        "    \n",
        "    # for each text column\n",
        "    for col in text_col:\n",
        "        neg = []\n",
        "        neu = []\n",
        "        pos = []\n",
        "        compound = []\n",
        "        # for each text\n",
        "        for text in df[col].values.flatten():\n",
        "            vs = analyzer.polarity_scores(text)\n",
        "            neg.append(vs[\"neg\"])\n",
        "            neu.append(vs[\"neu\"])\n",
        "            pos.append(vs[\"pos\"])\n",
        "            compound.append(vs[\"compound\"])\n",
        "    \n",
        "        # add column to dataframe\n",
        "        df[f\"neg_{col}\"] = neg\n",
        "        df[f\"neu_{col}\"] = neu\n",
        "        df[f\"pos_{col}\"] = pos\n",
        "        df[f\"compound_{col}\"] = compound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T11:07:46.643787Z",
          "iopub.status.busy": "2021-06-07T11:07:46.643439Z",
          "iopub.status.idle": "2021-06-07T11:07:52.874090Z",
          "shell.execute_reply": "2021-06-07T11:07:52.873198Z",
          "shell.execute_reply.started": "2021-06-07T11:07:46.643758Z"
        },
        "id": "lrBEPmDRv9C9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# add polarity score for each dataframe\n",
        "add_polarity_score(df_compi)\n",
        "add_polarity_score(df_compi_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCZcRVlLv9C9"
      },
      "source": [
        "# Train validation split on competition data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T11:08:21.688705Z",
          "iopub.status.busy": "2021-06-07T11:08:21.688226Z",
          "iopub.status.idle": "2021-06-07T11:08:21.697863Z",
          "shell.execute_reply": "2021-06-07T11:08:21.696685Z",
          "shell.execute_reply.started": "2021-06-07T11:08:21.688667Z"
        },
        "id": "cIFBuHJjv9C9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#### from sklearn.model_selection import train_test_split\n",
        "# split the data into train and valudation data(10%) with Stratify sampling, random state 33, \n",
        "X_train, X_val = train_test_split( df_compi, test_size=0.1, random_state=33)\n",
        "X_test = df_compi_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtK55SBiv9C9"
      },
      "source": [
        "# Creating BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T11:09:32.238009Z",
          "iopub.status.busy": "2021-06-07T11:09:32.237640Z",
          "iopub.status.idle": "2021-06-07T11:09:50.258661Z",
          "shell.execute_reply": "2021-06-07T11:09:50.257330Z",
          "shell.execute_reply.started": "2021-06-07T11:09:32.237972Z"
        },
        "id": "gnW2PnpIv9C-",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:277: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Loading the Pretrained Model from tensorflow HUB\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# maximum length of a seq in the data we have, for now i am making it as 300. You can change this\n",
        "max_seq_length = 300\n",
        "\n",
        "#BERT takes 3 inputs\n",
        "\n",
        "#this is input words. Sequence of words represented as integers\n",
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "\n",
        "#mask vector if you are padding anything\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n",
        "\n",
        "#segment vectors. If you are giving only one sentence for the classification, total seg vector is 0. \n",
        "#If you are giving two sentenced with [sep] token separated, first seq segment vectors are zeros and \n",
        "#second seq segment vector are 1's\n",
        "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "#bert layer \n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=False)\n",
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "\n",
        "#Bert model\n",
        "#We are using only pooled output not sequence out. \n",
        "#If you want to know about those, please read https://www.kaggle.com/questions-and-answers/86510\n",
        "bert_model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=pooled_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-06-07T11:09:50.260328Z",
          "iopub.status.busy": "2021-06-07T11:09:50.259976Z",
          "iopub.status.idle": "2021-06-07T11:09:50.276955Z",
          "shell.execute_reply": "2021-06-07T11:09:50.274524Z",
          "shell.execute_reply.started": "2021-06-07T11:09:50.260298Z"
        },
        "id": "NnZBWZrPv9C-",
        "outputId": "688db242-f603-42c8-ae1c-de091de876bb",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_word_ids (InputLayer  [(None, 300)]                0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " input_mask (InputLayer)     [(None, 300)]                0         []                            \n",
            "                                                                                                  \n",
            " segment_ids (InputLayer)    [(None, 300)]                0         []                            \n",
            "                                                                                                  \n",
            " keras_layer (KerasLayer)    [(None, 768),                1094822   ['input_word_ids[0][0]',      \n",
            "                              (None, 300, 768)]           41         'input_mask[0][0]',          \n",
            "                                                                     'segment_ids[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109482241 (417.64 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 109482241 (417.64 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "bert_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29qHYSdqv9C_"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T11:10:02.992295Z",
          "iopub.status.busy": "2021-06-07T11:10:02.991937Z",
          "iopub.status.idle": "2021-06-07T11:10:03.000325Z",
          "shell.execute_reply": "2021-06-07T11:10:02.998916Z",
          "shell.execute_reply.started": "2021-06-07T11:10:02.992266Z"
        },
        "id": "SW-6__fvv9C_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# getting Vocab file\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T11:10:03.240218Z",
          "iopub.status.busy": "2021-06-07T11:10:03.239830Z",
          "iopub.status.idle": "2021-06-07T11:10:03.335333Z",
          "shell.execute_reply": "2021-06-07T11:10:03.333996Z",
          "shell.execute_reply.started": "2021-06-07T11:10:03.240188Z"
        },
        "id": "6OchpIS0v9C_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Tokenization classes implementation.\n",
        "\n",
        "The file is forked from:\n",
        "https://github.com/google-research/bert/blob/master/tokenization.py.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "import six\n",
        "import tensorflow as tf\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "SPIECE_UNDERLINE = \"â–\"\n",
        "\n",
        "\n",
        "def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n",
        "  \"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"\n",
        "\n",
        "  # The casing has to be passed in by the user and there is no explicit check\n",
        "  # as to whether it matches the checkpoint. The casing information probably\n",
        "  # should have been stored in the bert_config.json file, but it's not, so\n",
        "  # we have to heuristically detect it to validate.\n",
        "\n",
        "  if not init_checkpoint:\n",
        "    return\n",
        "\n",
        "  m = re.match(\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\", init_checkpoint)\n",
        "  if m is None:\n",
        "    return\n",
        "\n",
        "  model_name = m.group(1)\n",
        "\n",
        "  lower_models = [\n",
        "      \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n",
        "      \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n",
        "  ]\n",
        "\n",
        "  cased_models = [\n",
        "      \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n",
        "      \"multi_cased_L-12_H-768_A-12\"\n",
        "  ]\n",
        "\n",
        "  is_bad_config = False\n",
        "  if model_name in lower_models and not do_lower_case:\n",
        "    is_bad_config = True\n",
        "    actual_flag = \"False\"\n",
        "    case_name = \"lowercased\"\n",
        "    opposite_flag = \"True\"\n",
        "\n",
        "  if model_name in cased_models and do_lower_case:\n",
        "    is_bad_config = True\n",
        "    actual_flag = \"True\"\n",
        "    case_name = \"cased\"\n",
        "    opposite_flag = \"False\"\n",
        "\n",
        "  if is_bad_config:\n",
        "    raise ValueError(\n",
        "        \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n",
        "        \"However, `%s` seems to be a %s model, so you \"\n",
        "        \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n",
        "        \"how the model was pre-training. If this error is wrong, please \"\n",
        "        \"just comment out this check.\" %\n",
        "        (actual_flag, init_checkpoint, model_name, case_name, opposite_flag))\n",
        "\n",
        "\n",
        "def convert_to_unicode(text):\n",
        "  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
        "  if six.PY3:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, bytes):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  elif six.PY2:\n",
        "    if isinstance(text, str):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    elif isinstance(text, unicode):\n",
        "      return text\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  else:\n",
        "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def printable_text(text):\n",
        "  \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
        "\n",
        "  # These functions want `str` for both Python2 and Python3, but in one case\n",
        "  # it's a Unicode string and in the other it's a byte string.\n",
        "  if six.PY3:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, bytes):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  elif six.PY2:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, unicode):\n",
        "      return text.encode(\"utf-8\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  else:\n",
        "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "  vocab = collections.OrderedDict()\n",
        "  index = 0\n",
        "  with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\n",
        "    while True:\n",
        "      token = convert_to_unicode(reader.readline())\n",
        "      if not token:\n",
        "        break\n",
        "      token = token.strip()\n",
        "      vocab[token] = index\n",
        "      index += 1\n",
        "  return vocab\n",
        "\n",
        "\n",
        "def convert_by_vocab(vocab, items):\n",
        "  \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n",
        "  output = []\n",
        "  for item in items:\n",
        "    output.append(vocab[item])\n",
        "  return output\n",
        "\n",
        "\n",
        "def convert_tokens_to_ids(vocab, tokens):\n",
        "  return convert_by_vocab(vocab, tokens)\n",
        "\n",
        "\n",
        "def convert_ids_to_tokens(inv_vocab, ids):\n",
        "  return convert_by_vocab(inv_vocab, ids)\n",
        "\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "  \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
        "  text = text.strip()\n",
        "  if not text:\n",
        "    return []\n",
        "  tokens = text.split()\n",
        "  return tokens\n",
        "\n",
        "\n",
        "class FullTokenizer(object):\n",
        "  \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_file, do_lower_case=True, split_on_punc=True):\n",
        "    self.vocab = load_vocab(vocab_file)\n",
        "    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
        "    self.basic_tokenizer = BasicTokenizer(\n",
        "        do_lower_case=do_lower_case, split_on_punc=split_on_punc)\n",
        "    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    split_tokens = []\n",
        "    for token in self.basic_tokenizer.tokenize(text):\n",
        "      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "        split_tokens.append(sub_token)\n",
        "\n",
        "    return split_tokens\n",
        "\n",
        "  def convert_tokens_to_ids(self, tokens):\n",
        "    return convert_by_vocab(self.vocab, tokens)\n",
        "\n",
        "  def convert_ids_to_tokens(self, ids):\n",
        "    return convert_by_vocab(self.inv_vocab, ids)\n",
        "\n",
        "\n",
        "class BasicTokenizer(object):\n",
        "  \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "  def __init__(self, do_lower_case=True, split_on_punc=True):\n",
        "    \"\"\"Constructs a BasicTokenizer.\n",
        "\n",
        "    Args:\n",
        "      do_lower_case: Whether to lower case the input.\n",
        "      split_on_punc: Whether to apply split on punctuations. By default BERT\n",
        "        starts a new token for punctuations. This makes detokenization difficult\n",
        "        for tasks like seq2seq decoding.\n",
        "    \"\"\"\n",
        "    self.do_lower_case = do_lower_case\n",
        "    self.split_on_punc = split_on_punc\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    \"\"\"Tokenizes a piece of text.\"\"\"\n",
        "    text = convert_to_unicode(text)\n",
        "    text = self._clean_text(text)\n",
        "\n",
        "    # This was added on November 1st, 2018 for the multilingual and Chinese\n",
        "    # models. This is also applied to the English models now, but it doesn't\n",
        "    # matter since the English models were not trained on any Chinese data\n",
        "    # and generally don't have any Chinese data in them (there are Chinese\n",
        "    # characters in the vocabulary because Wikipedia does have some Chinese\n",
        "    # words in the English Wikipedia.).\n",
        "    text = self._tokenize_chinese_chars(text)\n",
        "\n",
        "    orig_tokens = whitespace_tokenize(text)\n",
        "    split_tokens = []\n",
        "    for token in orig_tokens:\n",
        "      if self.do_lower_case:\n",
        "        token = token.lower()\n",
        "        token = self._run_strip_accents(token)\n",
        "      if self.split_on_punc:\n",
        "        split_tokens.extend(self._run_split_on_punc(token))\n",
        "      else:\n",
        "        split_tokens.append(token)\n",
        "\n",
        "    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "    return output_tokens\n",
        "\n",
        "  def _run_strip_accents(self, text):\n",
        "    \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "    text = unicodedata.normalize(\"NFD\", text)\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cat = unicodedata.category(char)\n",
        "      if cat == \"Mn\":\n",
        "        continue\n",
        "      output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "  def _run_split_on_punc(self, text):\n",
        "    \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "    chars = list(text)\n",
        "    i = 0\n",
        "    start_new_word = True\n",
        "    output = []\n",
        "    while i < len(chars):\n",
        "      char = chars[i]\n",
        "      if _is_punctuation(char):\n",
        "        output.append([char])\n",
        "        start_new_word = True\n",
        "      else:\n",
        "        if start_new_word:\n",
        "          output.append([])\n",
        "        start_new_word = False\n",
        "        output[-1].append(char)\n",
        "      i += 1\n",
        "\n",
        "    return [\"\".join(x) for x in output]\n",
        "\n",
        "  def _tokenize_chinese_chars(self, text):\n",
        "    \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cp = ord(char)\n",
        "      if self._is_chinese_char(cp):\n",
        "        output.append(\" \")\n",
        "        output.append(char)\n",
        "        output.append(\" \")\n",
        "      else:\n",
        "        output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "  def _is_chinese_char(self, cp):\n",
        "    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
        "    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
        "    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
        "    #\n",
        "    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
        "    # despite its name. The modern Korean Hangul alphabet is a different block,\n",
        "    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
        "    # space-separated words, so they are not treated specially and handled\n",
        "    # like the all of the other languages.\n",
        "    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
        "        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
        "        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
        "        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
        "        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
        "        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
        "        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
        "        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "  def _clean_text(self, text):\n",
        "    \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cp = ord(char)\n",
        "      if cp == 0 or cp == 0xfffd or _is_control(char):\n",
        "        continue\n",
        "      if _is_whitespace(char):\n",
        "        output.append(\" \")\n",
        "      else:\n",
        "        output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "\n",
        "class WordpieceTokenizer(object):\n",
        "  \"\"\"Runs WordPiece tokenziation.\"\"\"\n",
        "\n",
        "  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
        "    self.vocab = vocab\n",
        "    self.unk_token = unk_token\n",
        "    self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    \"\"\"Tokenizes a piece of text into its word pieces.\n",
        "\n",
        "    This uses a greedy longest-match-first algorithm to perform tokenization\n",
        "    using the given vocabulary.\n",
        "\n",
        "    For example:\n",
        "      input = \"unaffable\"\n",
        "      output = [\"un\", \"##aff\", \"##able\"]\n",
        "\n",
        "    Args:\n",
        "      text: A single token or whitespace separated tokens. This should have\n",
        "        already been passed through `BasicTokenizer.\n",
        "\n",
        "    Returns:\n",
        "      A list of wordpiece tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    text = convert_to_unicode(text)\n",
        "\n",
        "    output_tokens = []\n",
        "    for token in whitespace_tokenize(text):\n",
        "      chars = list(token)\n",
        "      if len(chars) > self.max_input_chars_per_word:\n",
        "        output_tokens.append(self.unk_token)\n",
        "        continue\n",
        "\n",
        "      is_bad = False\n",
        "      start = 0\n",
        "      sub_tokens = []\n",
        "      while start < len(chars):\n",
        "        end = len(chars)\n",
        "        cur_substr = None\n",
        "        while start < end:\n",
        "          substr = \"\".join(chars[start:end])\n",
        "          if start > 0:\n",
        "            substr = \"##\" + substr\n",
        "          if substr in self.vocab:\n",
        "            cur_substr = substr\n",
        "            break\n",
        "          end -= 1\n",
        "        if cur_substr is None:\n",
        "          is_bad = True\n",
        "          break\n",
        "        sub_tokens.append(cur_substr)\n",
        "        start = end\n",
        "\n",
        "      if is_bad:\n",
        "        output_tokens.append(self.unk_token)\n",
        "      else:\n",
        "        output_tokens.extend(sub_tokens)\n",
        "    return output_tokens\n",
        "\n",
        "\n",
        "def _is_whitespace(char):\n",
        "  \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "  # \\t, \\n, and \\r are technically control characters but we treat them\n",
        "  # as whitespace since they are generally considered as such.\n",
        "  if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    return True\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat == \"Zs\":\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def _is_control(char):\n",
        "  \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "  # These are technically control characters but we count them as whitespace\n",
        "  # characters.\n",
        "  if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    return False\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat in (\"Cc\", \"Cf\"):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def _is_punctuation(char):\n",
        "  \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "  cp = ord(char)\n",
        "  # We treat all non-letter/number ASCII as punctuation.\n",
        "  # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "  # Punctuation class but we treat them as punctuation anyways, for\n",
        "  # consistency.\n",
        "  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
        "    return True\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat.startswith(\"P\"):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def preprocess_text(inputs, remove_space=True, lower=False):\n",
        "  \"\"\"Preprocesses data by removing extra space and normalize data.\n",
        "\n",
        "  This method is used together with sentence piece tokenizer and is forked from:\n",
        "  https://github.com/google-research/google-research/blob/master/albert/tokenization.py\n",
        "\n",
        "  Args:\n",
        "    inputs: The input text.\n",
        "    remove_space: Whether to remove the extra space.\n",
        "    lower: Whether to lowercase the text.\n",
        "\n",
        "  Returns:\n",
        "    The preprocessed text.\n",
        "\n",
        "  \"\"\"\n",
        "  outputs = inputs\n",
        "  if remove_space:\n",
        "    outputs = \" \".join(inputs.strip().split())\n",
        "\n",
        "  if six.PY2 and isinstance(outputs, str):\n",
        "    try:\n",
        "      outputs = six.ensure_text(outputs, \"utf-8\")\n",
        "    except UnicodeDecodeError:\n",
        "      outputs = six.ensure_text(outputs, \"latin-1\")\n",
        "\n",
        "  outputs = unicodedata.normalize(\"NFKD\", outputs)\n",
        "  outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
        "  if lower:\n",
        "    outputs = outputs.lower()\n",
        "\n",
        "  return outputs\n",
        "\n",
        "\n",
        "def encode_pieces(sp_model, text, sample=False):\n",
        "  \"\"\"Segements text into pieces.\n",
        "\n",
        "  This method is used together with sentence piece tokenizer and is forked from:\n",
        "  https://github.com/google-research/google-research/blob/master/albert/tokenization.py\n",
        "\n",
        "\n",
        "  Args:\n",
        "    sp_model: A spm.SentencePieceProcessor object.\n",
        "    text: The input text to be segemented.\n",
        "    sample: Whether to randomly sample a segmentation output or return a\n",
        "      deterministic one.\n",
        "\n",
        "  Returns:\n",
        "    A list of token pieces.\n",
        "  \"\"\"\n",
        "  if six.PY2 and isinstance(text, six.text_type):\n",
        "    text = six.ensure_binary(text, \"utf-8\")\n",
        "\n",
        "  if not sample:\n",
        "    pieces = sp_model.EncodeAsPieces(text)\n",
        "  else:\n",
        "    pieces = sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
        "  new_pieces = []\n",
        "  for piece in pieces:\n",
        "    piece = printable_text(piece)\n",
        "    if len(piece) > 1 and piece[-1] == \",\" and piece[-2].isdigit():\n",
        "      cur_pieces = sp_model.EncodeAsPieces(piece[:-1].replace(\n",
        "          SPIECE_UNDERLINE, \"\"))\n",
        "      if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
        "        if len(cur_pieces[0]) == 1:\n",
        "          cur_pieces = cur_pieces[1:]\n",
        "        else:\n",
        "          cur_pieces[0] = cur_pieces[0][1:]\n",
        "      cur_pieces.append(piece[-1])\n",
        "      new_pieces.extend(cur_pieces)\n",
        "    else:\n",
        "      new_pieces.append(piece)\n",
        "\n",
        "  return new_pieces\n",
        "\n",
        "\n",
        "def encode_ids(sp_model, text, sample=False):\n",
        "  \"\"\"Segments text and return token ids.\n",
        "\n",
        "  This method is used together with sentence piece tokenizer and is forked from:\n",
        "  https://github.com/google-research/google-research/blob/master/albert/tokenization.py\n",
        "\n",
        "  Args:\n",
        "    sp_model: A spm.SentencePieceProcessor object.\n",
        "    text: The input text to be segemented.\n",
        "    sample: Whether to randomly sample a segmentation output or return a\n",
        "      deterministic one.\n",
        "\n",
        "  Returns:\n",
        "    A list of token ids.\n",
        "  \"\"\"\n",
        "  pieces = encode_pieces(sp_model, text, sample=sample)\n",
        "  ids = [sp_model.PieceToId(piece) for piece in pieces]\n",
        "  return ids\n",
        "\n",
        "\n",
        "class FullSentencePieceTokenizer(object):\n",
        "  \"\"\"Runs end-to-end sentence piece tokenization.\n",
        "\n",
        "  The interface of this class is intended to keep the same as above\n",
        "  `FullTokenizer` class for easier usage.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, sp_model_file):\n",
        "    \"\"\"Inits FullSentencePieceTokenizer.\n",
        "\n",
        "    Args:\n",
        "      sp_model_file: The path to the sentence piece model file.\n",
        "    \"\"\"\n",
        "    self.sp_model = spm.SentencePieceProcessor()\n",
        "    self.sp_model.Load(sp_model_file)\n",
        "    self.vocab = {\n",
        "        self.sp_model.IdToPiece(i): i\n",
        "        for i in six.moves.range(self.sp_model.GetPieceSize())\n",
        "    }\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    \"\"\"Tokenizes text into pieces.\"\"\"\n",
        "    return encode_pieces(self.sp_model, text)\n",
        "\n",
        "  def convert_tokens_to_ids(self, tokens):\n",
        "    \"\"\"Converts a list of tokens to a list of ids.\"\"\"\n",
        "    return [self.sp_model.PieceToId(printable_text(token)) for token in tokens]\n",
        "\n",
        "  def convert_ids_to_tokens(self, ids):\n",
        "    \"\"\"Converts a list of ids ot a list of tokens.\"\"\"\n",
        "    return [self.sp_model.IdToPiece(id_) for id_ in ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T11:10:05.595089Z",
          "iopub.status.busy": "2021-06-07T11:10:05.594710Z",
          "iopub.status.idle": "2021-06-07T11:10:05.733467Z",
          "shell.execute_reply": "2021-06-07T11:10:05.732110Z",
          "shell.execute_reply.started": "2021-06-07T11:10:05.595059Z"
        },
        "id": "23nQ9deov9DC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create tokenizer \" Instantiate FullTokenizer\"\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
        "# name must be \"tokenizer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T11:10:59.916268Z",
          "iopub.status.busy": "2021-06-07T11:10:59.915823Z",
          "iopub.status.idle": "2021-06-07T11:11:14.614951Z",
          "shell.execute_reply": "2021-06-07T11:11:14.613965Z",
          "shell.execute_reply.started": "2021-06-07T11:10:59.916232Z"
        },
        "id": "0RYDC-Ydv9DD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Create train and test tokens (X_train_tokens, X_test_tokens) from (X_train, X_test) using Tokenizer and \n",
        "add '[CLS]' at start of the Tokens and '[SEP]' at the end of the tokens.\n",
        "'''\n",
        "def get_tokens_mask_segment(Text):\n",
        "  # get the tokens\n",
        "  tokens = tokenizer.tokenize(Text)\n",
        "  \n",
        "  # marking begining and end of the sentence by prepending and appending delimiters\n",
        "  tokens = [\"[CLS]\", *tokens, \"[SEP]\"]\n",
        "  \n",
        "  # maximum number of tokens is 55(We already given this to BERT layer above) so shape is (None, 55)\n",
        "  # if it is less than 55, add '[PAD]' token else truncate the tokens length.(similar to padding)\n",
        "  # Based on padding, create the mask for Train and Test ( 1 for real token, 0 for '[PAD]'), \n",
        "  # it will also same shape as input tokens (None, 55) save those in X_train_mask, X_test_mask\n",
        "  l = len(tokens)\n",
        "  off_by = max_seq_length - len(tokens)\n",
        "  if off_by >0:\n",
        "    tokens = np.array(tokens + [\"[PAD]\"] * off_by)\n",
        "    mask = np.array([1] * l + ([0] * off_by))\n",
        "  else:\n",
        "    tokens = np.array(tokens[:max_seq_length-1] + [\"[SEP]\"])\n",
        "    mask = np.array([1] * max_seq_length)\n",
        "  \n",
        "  segment = np.array([0] * max_seq_length)\n",
        "\n",
        "  return tokens, mask, segment\n",
        "\n",
        "# -------------------------text column-------------\n",
        "X_train_tokens_text, X_train_mask_text, X_train_segment_text = [[], [], []]\n",
        "\n",
        "X_val_tokens_text, X_val_mask_text, X_val_segment_text = [[], [], []]\n",
        "\n",
        "X_test_tokens_text, X_test_mask_text, X_test_segment_text = [[], [], []]\n",
        "\n",
        "X_kaggle_tokens_text, X_kaggle_mask_text, X_kaggle_segment_text = [[], [], []]\n",
        "\n",
        "# -------------------------parent text-----------\n",
        "\n",
        "X_train_tokens_parent_text, X_train_mask_parent_text, X_train_segment_parent_text = [[], [], []]\n",
        "\n",
        "X_val_tokens_parent_text, X_val_mask_parent_text, X_val_segment_parent_text = [[], [], []]\n",
        "\n",
        "X_test_tokens_parent_text, X_test_mask_parent_text, X_test_segment_parent_text = [[], [], []]\n",
        "\n",
        "X_kaggle_tokens_parent_text, X_kaggle_mask_parent_text, X_kaggle_segment_parent_text = [[], [], []]\n",
        "\n",
        "# ------------------text column------------------\n",
        "\n",
        "for text in X_train.text.values:\n",
        "  tokens, mask, segment = get_tokens_mask_segment(text)\n",
        "  tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  X_train_tokens_text.append(tokens)\n",
        "  X_train_mask_text.append(mask)\n",
        "  X_train_segment_text.append(segment)\n",
        "\n",
        "for text in X_val.text.values:\n",
        "  tokens, mask, segment = get_tokens_mask_segment(text)\n",
        "  tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  X_val_tokens_text.append(tokens)\n",
        "  X_val_mask_text.append(mask)\n",
        "  X_val_segment_text.append(segment)\n",
        "\n",
        "for text in X_test.text.values:\n",
        "  tokens, mask, segment = get_tokens_mask_segment(text)\n",
        "  tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  X_test_tokens_text.append(tokens)\n",
        "  X_test_mask_text.append(mask)\n",
        "  X_test_segment_text.append(segment)\n",
        "\n",
        "# ------------------parent text column------------------\n",
        "for parent_text in X_train.parent_text.values:\n",
        "  tokens, mask, segment = get_tokens_mask_segment(parent_text)\n",
        "  tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  X_train_tokens_parent_text.append(tokens)\n",
        "  X_train_mask_parent_text.append(mask)\n",
        "  X_train_segment_parent_text.append(segment)\n",
        "\n",
        "for parent_text in X_val.parent_text.values:\n",
        "  tokens, mask, segment = get_tokens_mask_segment(parent_text)\n",
        "  tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  X_val_tokens_parent_text.append(tokens)\n",
        "  X_val_mask_parent_text.append(mask)\n",
        "  X_val_segment_parent_text.append(segment)\n",
        "\n",
        "for parent_text in X_test.parent_text.values:\n",
        "  tokens, mask, segment = get_tokens_mask_segment(parent_text)\n",
        "  tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  X_test_tokens_parent_text.append(tokens)\n",
        "  X_test_mask_parent_text.append(mask)\n",
        "  X_test_segment_parent_text.append(segment)\n",
        "\n",
        "# type cast to numpy array\n",
        "\n",
        "# --------------------text column---------------\n",
        "X_train_tokens_text = np.array(X_train_tokens_text)\n",
        "X_train_mask_text = np.array(X_train_mask_text)\n",
        "X_train_segment_text = np.array(X_train_segment_text)\n",
        "\n",
        "\n",
        "X_val_tokens_text = np.array(X_val_tokens_text)\n",
        "X_val_mask_text = np.array(X_val_mask_text)\n",
        "X_val_segment_text = np.array(X_val_segment_text)\n",
        "\n",
        "X_test_tokens_text = np.array(X_test_tokens_text)\n",
        "X_test_mask_text = np.array(X_test_mask_text)\n",
        "X_test_segment_text = np.array(X_test_segment_text)\n",
        "\n",
        "#------------------------parent text--------------\n",
        "X_train_tokens_parent_text = np.array(X_train_tokens_parent_text)\n",
        "X_train_mask_parent_text = np.array(X_train_mask_parent_text)\n",
        "X_train_segment_parent_text = np.array(X_train_segment_parent_text)\n",
        "\n",
        "\n",
        "X_val_tokens_parent_text = np.array(X_val_tokens_parent_text)\n",
        "X_val_mask_parent_text = np.array(X_val_mask_parent_text)\n",
        "X_val_segment_parent_text = np.array(X_val_segment_parent_text)\n",
        "\n",
        "X_test_tokens_parent_text = np.array(X_test_tokens_parent_text)\n",
        "X_test_mask_parent_text = np.array(X_test_mask_parent_text)\n",
        "X_test_segment_parent_text = np.array(X_test_segment_parent_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8BaHSmEv9DE"
      },
      "source": [
        "# Getting Embeddings from BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T11:11:49.133373Z",
          "iopub.status.busy": "2021-06-07T11:11:49.132979Z",
          "iopub.status.idle": "2021-06-07T11:51:02.133284Z",
          "shell.execute_reply": "2021-06-07T11:51:02.132444Z",
          "shell.execute_reply.started": "2021-06-07T11:11:49.133339Z"
        },
        "id": "2hgG-WtUv9DE",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "71/71 [==============================] - 8900s 125s/step\n",
            "8/8 [==============================] - 961s 116s/step\n",
            "16/16 [==============================] - 2250s 140s/step\n",
            "71/71 [==============================] - 10912s 153s/step\n",
            "8/8 [==============================] - 1559s 180s/step\n",
            "16/16 [==============================] - 3829s 235s/step\n"
          ]
        }
      ],
      "source": [
        "# -----------------------text column------------------\n",
        "# get the train, validation and test embeddings for \"text\" column\n",
        "X_train_pooled_output_text = bert_model.predict([X_train_tokens_text, X_train_mask_text, X_train_segment_text], batch_size= 64)\n",
        "X_val_pooled_output_text = bert_model.predict([X_val_tokens_text, X_val_mask_text, X_val_segment_text], batch_size= 64)\n",
        "X_test_pooled_output_text = bert_model.predict([X_test_tokens_text, X_test_mask_text, X_test_segment_text], batch_size= 64)\n",
        "\n",
        "# -----------------------parent text --------------------\n",
        "# get the train, validation and test embeddings for \"parent text\" column\n",
        "X_train_pooled_output_parent_text = bert_model.predict([X_train_tokens_parent_text, X_train_mask_parent_text, X_train_segment_parent_text], batch_size= 64)\n",
        "X_val_pooled_output_parent_text = bert_model.predict([X_val_tokens_parent_text, X_val_mask_parent_text, X_val_segment_parent_text], batch_size= 64)\n",
        "X_test_pooled_output_parent_text = bert_model.predict([X_test_tokens_parent_text, X_test_mask_parent_text, X_test_segment_parent_text], batch_size= 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**\n",
        "\n",
        "Expected Time taken to run above cell is 7 Hours 54 min (474m 21.5s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAvGyaJpv9DE"
      },
      "source": [
        "# Saving embedding to the disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T11:53:18.544881Z",
          "iopub.status.busy": "2021-06-07T11:53:18.544542Z",
          "iopub.status.idle": "2021-06-07T11:53:18.635921Z",
          "shell.execute_reply": "2021-06-07T11:53:18.634349Z",
          "shell.execute_reply.started": "2021-06-07T11:53:18.544851Z"
        },
        "id": "hJhH5rZFv9DE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pickle.dump((X_train_pooled_output_text, X_val_pooled_output_text, X_test_pooled_output_text),open(('embeddings_text.pkl'),'wb'))\n",
        "pickle.dump((X_train_pooled_output_parent_text, X_val_pooled_output_parent_text, X_test_pooled_output_parent_text),open(('embeddings_parent_text.pkl'),'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Ya_FWAv9DE"
      },
      "source": [
        "# Load saved embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T12:38:20.756084Z",
          "iopub.status.busy": "2021-06-07T12:38:20.755632Z",
          "iopub.status.idle": "2021-06-07T12:38:20.796319Z",
          "shell.execute_reply": "2021-06-07T12:38:20.795327Z",
          "shell.execute_reply.started": "2021-06-07T12:38:20.756048Z"
        },
        "id": "EvgQ8ttuv9DF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# X_train_pooled_output,X_val_pooled_output, X_test_pooled_output = pickle.load(open(('embeddings_text.pkl'), 'rb'))\n",
        "# X_train_pooled_output_parent_text, X_val_pooled_output_parent_text, X_test_pooled_output_parent_text = pickle.load(open(('embeddings_parent_text.pkl'), 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_mUEXR_v9DF"
      },
      "source": [
        "# Append remaining features\n",
        "We will append embeddings + numeric features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T12:04:44.960021Z",
          "iopub.status.busy": "2021-06-07T12:04:44.959521Z",
          "iopub.status.idle": "2021-06-07T12:04:45.012545Z",
          "shell.execute_reply": "2021-06-07T12:04:45.011703Z",
          "shell.execute_reply.started": "2021-06-07T12:04:44.959989Z"
        },
        "id": "1UjN3ruav9DF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def add_text_length(df, compi= 1):\n",
        "    \"\"\"\n",
        "    add word count feature\n",
        "    \"\"\"\n",
        "    # adding length column\n",
        "    df[\"text_len\"] = df.text.apply(lambda x: len(x.split()))\n",
        "    df[\"parent_text_len\"] = df.parent_text.apply(lambda x: len(x.split()))\n",
        "    return df\n",
        "\n",
        "X_train = add_text_length(X_train.copy())\n",
        "X_val = add_text_length(X_val.copy())\n",
        "X_test = add_text_length(X_test.copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T12:07:46.122747Z",
          "iopub.status.busy": "2021-06-07T12:07:46.122349Z",
          "iopub.status.idle": "2021-06-07T12:07:46.135361Z",
          "shell.execute_reply": "2021-06-07T12:07:46.134257Z",
          "shell.execute_reply.started": "2021-06-07T12:07:46.122706Z"
        },
        "id": "Te0Ir_8nv9DF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# append remaining features\n",
        "num_train = X_train[['controversiality', 'parent_score',\n",
        "       'parent_controversiality', 'text_len', 'parent_text_len']]\n",
        "\n",
        "num_val = X_val[['controversiality', 'parent_score',\n",
        "       'parent_controversiality', 'text_len', 'parent_text_len']]\n",
        "\n",
        "num_test = X_test[['controversiality', 'parent_score',\n",
        "       'parent_controversiality', 'text_len', 'parent_text_len']]\n",
        "\n",
        "# sentiment feature list\n",
        "sentiment_score_list = [\"neg_text\",\n",
        "                        \"neu_text\",\n",
        "                        \"pos_text\",\n",
        "                        \"compound_text\",\n",
        "                        \"neg_parent_text\",\n",
        "                        \"neu_parent_text\",\n",
        "                        \"pos_parent_text\",\n",
        "                        \"compound_parent_text\"]\n",
        "train_sentiment = X_train[sentiment_score_list].values\n",
        "test_sentiment = X_test[sentiment_score_list].values\n",
        "val_sentiment = X_val[sentiment_score_list].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T12:13:26.592821Z",
          "iopub.status.busy": "2021-06-07T12:13:26.592235Z",
          "iopub.status.idle": "2021-06-07T12:13:26.628467Z",
          "shell.execute_reply": "2021-06-07T12:13:26.627654Z",
          "shell.execute_reply.started": "2021-06-07T12:13:26.592787Z"
        },
        "id": "2YvnMnPfv9DF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "X_train_final = np.concatenate([X_train_pooled_output_text, X_train_pooled_output_parent_text, num_train, train_sentiment], axis= 1)\n",
        "X_val_final = np.concatenate([X_val_pooled_output_text, X_val_pooled_output_parent_text, num_val, val_sentiment], axis= 1)\n",
        "X_test_final = np.concatenate([X_test_pooled_output_text, X_test_pooled_output_parent_text, num_test, test_sentiment], axis= 1)\n",
        "\n",
        "y_train = X_train.Score\n",
        "y_val = X_val.Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-06-07T12:13:28.488340Z",
          "iopub.status.busy": "2021-06-07T12:13:28.487701Z",
          "iopub.status.idle": "2021-06-07T12:13:28.568158Z",
          "shell.execute_reply": "2021-06-07T12:13:28.567177Z",
          "shell.execute_reply.started": "2021-06-07T12:13:28.488284Z"
        },
        "id": "EUutNAsJv9DG",
        "outputId": "76f5a060-a466-40b4-9ae1-110e13bffb3f",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               198400    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 209409 (818.00 KB)\n",
            "Trainable params: 209409 (818.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "NN_model = Sequential()\n",
        "\n",
        "# The Input Layer :\n",
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train_final.shape[1], activation='relu'))\n",
        "\n",
        "# The Hidden Layers :\n",
        "NN_model.add(Dense(64, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(32, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(16, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(8, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# The Output Layer :\n",
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
        "op = Adam(lr= 0.0001)\n",
        "# Compile the network :\n",
        "NN_model.compile(loss='MSE', optimizer=op)\n",
        "NN_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T12:13:28.794804Z",
          "iopub.status.busy": "2021-06-07T12:13:28.794408Z",
          "iopub.status.idle": "2021-06-07T12:13:28.801960Z",
          "shell.execute_reply": "2021-06-07T12:13:28.800785Z",
          "shell.execute_reply.started": "2021-06-07T12:13:28.794773Z"
        },
        "id": "7ZvAxftEv9DG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# callbacks and checkpoints\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "checkpoint_path = \"best.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "callback_list = [\n",
        "ModelCheckpoint(checkpoint_path, monitor = 'val_loss',verbose = 1,save_weights_only=True, save_best_only = True,mode=\"min\"),\n",
        "EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min'),\n",
        "ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='min',min_delta=1e-4)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-06-07T12:13:34.234508Z",
          "iopub.status.busy": "2021-06-07T12:13:34.234150Z",
          "iopub.status.idle": "2021-06-07T12:13:38.886042Z",
          "shell.execute_reply": "2021-06-07T12:13:38.884651Z",
          "shell.execute_reply.started": "2021-06-07T12:13:34.234478Z"
        },
        "id": "OXU9AdXnv9DG",
        "outputId": "22020a7a-2fbd-47ee-9beb-71b459b7d77d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "WARNING:tensorflow:From c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "141/141 [==============================] - ETA: 0s - loss: 38148.7266\n",
            "Epoch 1: val_loss improved from inf to 22899.44727, saving model to best.ckpt\n",
            "141/141 [==============================] - 90s 329ms/step - loss: 38148.7266 - val_loss: 22899.4473 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "140/141 [============================>.] - ETA: 0s - loss: 29280.2227\n",
            "Epoch 2: val_loss improved from 22899.44727 to 21117.12695, saving model to best.ckpt\n",
            "141/141 [==============================] - 38s 271ms/step - loss: 29209.1367 - val_loss: 21117.1270 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "140/141 [============================>.] - ETA: 0s - loss: 29085.8555\n",
            "Epoch 3: val_loss improved from 21117.12695 to 20905.56836, saving model to best.ckpt\n",
            "141/141 [==============================] - 108s 772ms/step - loss: 29249.1973 - val_loss: 20905.5684 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "141/141 [==============================] - ETA: 0s - loss: 28424.1289\n",
            "Epoch 4: val_loss improved from 20905.56836 to 19513.65820, saving model to best.ckpt\n",
            "141/141 [==============================] - 46s 331ms/step - loss: 28424.1289 - val_loss: 19513.6582 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "139/141 [============================>.] - ETA: 0s - loss: 28843.0371\n",
            "Epoch 5: val_loss did not improve from 19513.65820\n",
            "141/141 [==============================] - 42s 300ms/step - loss: 28568.0820 - val_loss: 21269.5098 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "140/141 [============================>.] - ETA: 0s - loss: 28512.0859\n",
            "Epoch 6: val_loss did not improve from 19513.65820\n",
            "141/141 [==============================] - 32s 229ms/step - loss: 28504.1484 - val_loss: 22734.0391 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "139/141 [============================>.] - ETA: 0s - loss: 27825.3105\n",
            "Epoch 7: val_loss did not improve from 19513.65820\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 28170.9277 - val_loss: 20548.2402 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "141/141 [==============================] - ETA: 0s - loss: 27885.0527\n",
            "Epoch 8: val_loss did not improve from 19513.65820\n",
            "141/141 [==============================] - 21s 151ms/step - loss: 27885.0527 - val_loss: 19721.4668 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "137/141 [============================>.] - ETA: 0s - loss: 27631.8359\n",
            "Epoch 9: val_loss did not improve from 19513.65820\n",
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "141/141 [==============================] - 27s 193ms/step - loss: 28087.4824 - val_loss: 19687.3770 - lr: 0.0010\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x266ffebb1d0>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fit the data\n",
        "NN_model.fit(X_train_final, \n",
        "             y_train, \n",
        "             validation_data=(X_val_final,  y_val), \n",
        "             epochs=500, \n",
        "             batch_size=32, \n",
        "             callbacks=callback_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ-Mh-khv9DG"
      },
      "source": [
        "# Making predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-07T12:13:53.184056Z",
          "iopub.status.busy": "2021-06-07T12:13:53.183700Z",
          "iopub.status.idle": "2021-06-07T12:13:53.343905Z",
          "shell.execute_reply": "2021-06-07T12:13:53.342800Z",
          "shell.execute_reply.started": "2021-06-07T12:13:53.184010Z"
        },
        "id": "VtvHv88yv9DG",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 20s 9ms/step\n"
          ]
        }
      ],
      "source": [
        "# predictions\n",
        "pred = NN_model.predict(X_test_final)\n",
        "# to dataframe\n",
        "df_submit = pd.DataFrame({\"prediction\": pred.flatten()})\n",
        "# save to the disk\n",
        "df_submit.to_csv(\"submission.csv\", index= False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXjCiZnyv9DG"
      },
      "source": [
        "**The submission of this baseline model yields 190.6713 RMSE on public leaderboard**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "reddit_comment_score_prediction_no_external_data(colab).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
